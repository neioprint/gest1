<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script  src="face-api.min.js"></script>
    <title>Document</title>
    <style>
        /* img{
          display:none;
        } */
        body {
          margin: 0;
          padding: 0;
          width: 100vw;
          height: 100vh;
          display: flex;
          justify-content: center;
          align-items: center;
          flex-direction: column
        }
    
        canvas {
          position: absolute;
          top: 0;
          left: 0;
        }
      </style>
</head>
<body>
<canvas></canvas>
    <img src="./labeled_images/sidahmed.jpg" alt="" id="myImage">
    <script>

Promise.all([
  faceapi.nets.faceRecognitionNet.loadFromUri('/models'),
  faceapi.nets.faceLandmark68Net.loadFromUri('/models'),
  faceapi.nets.ssdMobilenetv1.loadFromUri('/models')
]).then(start)
  
  async function start() {   
    // const MODEL_URL = './models'
    // await faceapi.loadSsdMobilenetv1Model(MODEL_URL)
    // await faceapi.loadFaceLandmarkModel(MODEL_URL)
    // await faceapi.loadFaceRecognitionModel(MODEL_URL)
    console.log('Model loaded successed!')
    const input = document.getElementById('myImage')
    canvas = faceapi.createCanvasFromMedia(input)
    const displaySize = { width: input.width, height: input.height }
    let fullFaceDescriptions = await faceapi.detectAllFaces(input).withFaceLandmarks().withFaceDescriptors()
    
    fullFaceDescriptions = faceapi.resizeResults(fullFaceDescriptions,displaySize)

    faceapi.draw.drawDetections(canvas, fullFaceDescriptions)

   // faceapi.draw.drawLandmarks(canvas, fullFaceDescriptions)
    
    const labels = ['houari', 'sidahmed']
    
const labeledFaceDescriptors = await Promise.all(
  labels.map(async label => {
    // fetch image data from urls and convert blob to HTMLImage element
    const imgUrl = `./labeled_images/${label}.jpg`
    const img = await faceapi.fetchImage(imgUrl)
    
    // detect the face with the highest score in the image and compute it's landmarks and face descriptor
    const fullFaceDescription = await faceapi.detectSingleFace(img).withFaceLandmarks().withFaceDescriptor()
    
    if (!fullFaceDescription) {
      throw new Error(`no faces detected for ${label}`)
    }
    
    const faceDescriptors = [fullFaceDescription.descriptor]
    return new faceapi.LabeledFaceDescriptors(label, faceDescriptors)
  })
)
 
// 0.6 is a good distance threshold value to judge
// whether the descriptors match or not
const maxDescriptorDistance = 0.6
const faceMatcher = new faceapi.FaceMatcher(labeledFaceDescriptors, maxDescriptorDistance)

const results = fullFaceDescriptions.map(fd => faceMatcher.findBestMatch(fd.descriptor))


     
results.forEach((bestMatch, i) => {
  const box = fullFaceDescriptions[i].detection.box
  const text = bestMatch.toString()
  const drawBox = new faceapi.draw.DrawBox(box, { label: text })
  drawBox.draw(canvas)
})
 }
    </script>   
</body>
</html>
